{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['lines.linewidth'] = 3\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression summary\n",
    "\n",
    "The logistic or sigmoid function can be written two equivalent ways:\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + \\exp(-t)} = \\frac{\\exp(t)}{1 + \\exp(t)} $$\n",
    "\n",
    "The logistic regression model assumes the following probabilities of $Y \\in \\{0, 1\\}$ given column vector $X$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(Y=1|X) &= \\sigma(X^T \\beta) &&= \\frac{1}{1 + \\exp(-X^T \\beta)} &= \\frac{\\exp(X^T\\beta)}{1 + \\exp(X^T\\beta)} \\\\[10pt]\n",
    "P(Y=0|X) &= \\sigma(-X^T \\beta) &&= \\frac{1}{1 + \\exp(X^T \\beta)}  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The loss most typically used to fit $\\beta$ is the log loss or cross-entropy loss, which is the negative log probability of the correct (observed) $Y$ value. This loss for true $Y \\in \\{0, 1\\}$ and predicted probability $\\hat Y \\in [0, 1]$ is often written:\n",
    "\n",
    "$$-Y \\log(\\hat Y) - (1-Y)\\log(1- \\hat Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(t):\n",
    "    \"\"\"The logistic or sigmoid function, denoted Ïƒ(t).\n",
    "    \n",
    "    Note: This is actually a special case of what is generally \n",
    "          named the \"logistic\" function,\n",
    "          which allows for a different numerator and offset, \n",
    "          but lots of people call this the logistic function in practice.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def prediction(x, beta):\n",
    "    \"\"\"Prediction under the logistic model for features x and parameters b.\"\"\"\n",
    "    return sigma(x @ beta)\n",
    "\n",
    "def squared_loss(y, y_hat):\n",
    "    \"\"\"Squared loss applies to any true y and predicted y_hat.\"\"\"\n",
    "    return (y - y_hat) ** 2\n",
    "\n",
    "def log_loss(y, y_hat):\n",
    "    \"\"\"Log loss or cross-entropy loss, assuming y is in [0, 1].\"\"\"\n",
    "    assert y in [0, 1]\n",
    "    return -y * np.log(y_hat) - (1-y) * np.log(1-y_hat)\n",
    "\n",
    "def empirical_risk(true_ys, predicted_ys, loss):\n",
    "    \"\"\"The empirical risk is the average loss for a sample.\"\"\"\n",
    "    losses = [loss(y, y_hat) for y, y_hat in zip(true_ys, predicted_ys)]\n",
    "    return np.average(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Risk\n",
    "\n",
    "Filling in $\\hat Y = P(Y=1|X)$ and filling in the form of the model, we find different ways of expressing the same loss:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\beta) &= -Y \\log(\\hat Y) - (1-Y)\\log(1- \\hat Y) \\\\[10pt]\n",
    "         &= - Y \\log P(Y=1|X) - (1-Y) \\log P(Y=0|X)  \\\\[10pt]\n",
    "         &= - Y \\log \\frac{\\exp(X^T\\beta)}{1 + \\exp(X^T\\beta)} - (1-Y) \\log \\frac{1}{1 + \\exp(X^T\\beta)}  \\\\[10pt]\n",
    "         &= - Y (\\log(\\exp(X^T\\beta)) - \\log(1 + \\exp(X^T\\beta))) - (1-Y) (-\\log (1 + \\exp(X^T\\beta)))  \\\\[10pt]\n",
    "         &= - YX^T\\beta + Y \\log(1 + \\exp(X^T\\beta))) - Y \\log(1 + \\exp(X^T\\beta))) + \\log (1 + \\exp(X^T\\beta))  \\\\[10pt]\n",
    "         &= - YX^T\\beta + \\log (1 + \\exp(X^T\\beta)) \\\\[10pt]\n",
    "         &= -\\left(YX^T\\beta + \\log \\sigma(-X^T\\beta)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Where the last step follows from $\\log (1 + \\exp(X^T\\beta)) = -(- \\log (1 + \\exp(X^T\\beta))) = -\\log \\frac{1}{1 + \\exp(X^T\\beta)} = -\\log \\sigma(-X^T\\beta)$.\n",
    "\n",
    "The empirical risk (average loss across a sample) for a set of observations $(x_1, y_1) \\dots (x_n, y_n)$ is often written:\n",
    "\n",
    "$$R(\\beta, x, y) = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i x_i^T\\beta + \\log \\sigma(-x_i^T\\beta) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Gradient\n",
    "\n",
    "Using thelogistic regression model and log loss, find the gradient of the empirical risk.\n",
    "\n",
    "First, we compute the derivative of the sigmoid function since we'll use it in our gradient calculation.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(t) &= \\frac{1}{1 + e^{-t}} \\\\[10pt]\n",
    "\\sigma'(t) &= \\frac{e^{-t}}{(1 + e^{-t})^2} \\\\[10pt]\n",
    "\\sigma'(t) &= \\frac{1}{1 + e^{-t}} \\cdot \\left(1 - \\frac{1}{1 + e^{-t}} \\right) \\\\[10pt]\n",
    "\\sigma'(t) &= \\sigma(t) (1 - \\sigma(t))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a shorthand, we define $ \\sigma_i = \\sigma(-x_i^T \\beta) $. We will soon need the gradient of $ \\sigma_i $ with respect to the vector $ \\beta $ so we will derive it now using the chain rule. \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\beta} \\sigma_i\n",
    "&= \\nabla_{\\beta} \\sigma(-x_i^T \\beta) \\\\[10pt]\n",
    "&= \\sigma\\left(-x_i^T \\beta\\right) \\left(1 - \\sigma(-x_i^T \\beta)\\right)  \\nabla_{\\beta} \\left(-x_i^T \\beta\\right) \\\\[10pt]\n",
    "&= -\\sigma_i (1 - \\sigma_i) x_i \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we derive the gradient of the cross-entropy loss with respect to the model parameters $ \\boldsymbol{\\beta} $. We use the fact that $(1-\\sigma_i) = \\sigma(x_i^T\\beta)$, since $\\sigma(x^T\\beta) + \\sigma(-x^T\\beta) = 1$.\n",
    "\n",
    "\\begin{align*}\n",
    "R(\\beta, x, y) &= - \\frac{1}{n}\\sum_{i=1}^n \\left[ y_i x_i^T\\beta + \\log \\sigma_i \\right] \\\\[10pt]\n",
    "\\nabla_{\\beta} R(\\beta, x, y) &= - \\frac{1}{n}\\sum_{i=1}^n \\left( y_i x_i - \\frac{1}{\\sigma_i} \\sigma_i (1 - \\sigma_i) x_i \\right) \\\\[10pt]\n",
    "                              &= - \\frac{1}{n}\\sum_{i=1}^n \\left( y_i x_i - \\sigma(x_i^T\\beta) x_i \\right) \\\\[10pt]\n",
    "                              &= - \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\sigma(x_i^T\\beta)\\right) x_i  \\\\[10pt]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, beta0, gradient_function, max_iter=50000,  \n",
    "                     epsilon=1e-8, lr=0.1, clip=1):\n",
    "    \"\"\"Run gradient descent on a dataset (x, y) \n",
    "    with gradient clipping and learning rate decay.\"\"\"\n",
    "    beta = beta0\n",
    "    for t in range(1, max_iter):\n",
    "        grad = gradient_function(beta, x, y)\n",
    "        beta = beta - (lr/(1 + t/100)) * np.clip(grad, -clip, clip) \n",
    "        # Detect approximate convergence: small gradient\n",
    "        if np.linalg.norm(grad) < epsilon:\n",
    "            return beta\n",
    "    return beta\n",
    "\n",
    "def risk_gradient(beta, x, y):\n",
    "    \"\"\"Risk gradient for a whole dataset at once.\"\"\"\n",
    "    n = x.shape[0]\n",
    "    return -(1/n) * x @ (y - sigma(x.T @ beta)) \n",
    "\n",
    "def logistic_regression(x, y):\n",
    "    \"\"\"Train a logistic regression classifier using gradient descent.\"\"\"\n",
    "    beta0 = np.zeros(x.shape[0])\n",
    "    beta = gradient_descent(x, y, beta0, risk_gradient)\n",
    "    return beta    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                  17.990000\n",
       "mean texture                 10.380000\n",
       "mean perimeter              122.800000\n",
       "mean area                  1001.000000\n",
       "mean smoothness               0.118400\n",
       "mean compactness              0.277600\n",
       "mean concavity                0.300100\n",
       "mean concave points           0.147100\n",
       "mean symmetry                 0.241900\n",
       "mean fractal dimension        0.078710\n",
       "radius error                  1.095000\n",
       "texture error                 0.905300\n",
       "perimeter error               8.589000\n",
       "area error                  153.400000\n",
       "smoothness error              0.006399\n",
       "compactness error             0.049040\n",
       "concavity error               0.053730\n",
       "concave points error          0.015870\n",
       "symmetry error                0.030030\n",
       "fractal dimension error       0.006193\n",
       "worst radius                 25.380000\n",
       "worst texture                17.330000\n",
       "worst perimeter             184.600000\n",
       "worst area                 2019.000000\n",
       "worst smoothness              0.162200\n",
       "worst compactness             0.665600\n",
       "worst concavity               0.711900\n",
       "worst concave points          0.265400\n",
       "worst symmetry                0.460100\n",
       "worst fractal dimension       0.118900\n",
       "bias                          1.000000\n",
       "malignant                     1.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "cancer = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "cancer['bias'] = 1.0\n",
    "# Target data_dict['target'] = 0 is malignant; 1 is benign\n",
    "cancer['malignant'] = 1 - data_dict['target']\n",
    "cancer.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>bias</th>\n",
       "      <th>malignant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.0</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.372583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension     ...      worst perimeter  \\\n",
       "count     569.000000              569.000000     ...           569.000000   \n",
       "mean        0.181162                0.062798     ...           107.261213   \n",
       "std         0.027414                0.007060     ...            33.602542   \n",
       "min         0.106000                0.049960     ...            50.410000   \n",
       "25%         0.161900                0.057700     ...            84.110000   \n",
       "50%         0.179200                0.061540     ...            97.660000   \n",
       "75%         0.195700                0.066120     ...           125.400000   \n",
       "max         0.304000                0.097440     ...           251.200000   \n",
       "\n",
       "        worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "count   569.000000        569.000000         569.000000       569.000000   \n",
       "mean    880.583128          0.132369           0.254265         0.272188   \n",
       "std     569.356993          0.022832           0.157336         0.208624   \n",
       "min     185.200000          0.071170           0.027290         0.000000   \n",
       "25%     515.300000          0.116600           0.147200         0.114500   \n",
       "50%     686.500000          0.131300           0.211900         0.226700   \n",
       "75%    1084.000000          0.146000           0.339100         0.382900   \n",
       "max    4254.000000          0.222600           1.058000         1.252000   \n",
       "\n",
       "       worst concave points  worst symmetry  worst fractal dimension   bias  \\\n",
       "count            569.000000      569.000000               569.000000  569.0   \n",
       "mean               0.114606        0.290076                 0.083946    1.0   \n",
       "std                0.065732        0.061867                 0.018061    0.0   \n",
       "min                0.000000        0.156500                 0.055040    1.0   \n",
       "25%                0.064930        0.250400                 0.071460    1.0   \n",
       "50%                0.099930        0.282200                 0.080040    1.0   \n",
       "75%                0.161400        0.317900                 0.092080    1.0   \n",
       "max                0.291000        0.663800                 0.207500    1.0   \n",
       "\n",
       "        malignant  \n",
       "count  569.000000  \n",
       "mean     0.372583  \n",
       "std      0.483918  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  \n",
       "\n",
       "[8 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size:  426\n",
      "Test Data Size:  143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(cancer, test_size=0.25, random_state=100)\n",
    "print(\"Training Data Size: \", len(train))\n",
    "print(\"Test Data Size: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train['mean radius'], train['malignant']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "radii = np.linspace(5, 30, 50)\n",
    "averages = [np.average(train[np.abs(train['mean radius']-r)<2]['malignant']) for r in radii]\n",
    "plt.scatter(train['mean radius'], train['malignant']);\n",
    "plt.scatter(radii, averages, color='gold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(t):\n",
    "    return t[['bias', 'mean radius']].values.T\n",
    "    \n",
    "x_train, y_train = features(train), train['malignant'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-14.8970826 ,   1.01064211]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(fit_intercept=False, C=1e9, solver='lbfgs')\n",
    "model.fit(x_train.T, y_train)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(train['mean radius'], train['malignant']);\n",
    "plt.scatter(radii, averages, color='gold');\n",
    "plt.plot(radii, sigma(model.coef_[0,0] + radii * model.coef_[0,1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.86831235,   1.00862628])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = logistic_regression(x_train, y_train)\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00645167, -0.00045204])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_gradient(beta, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/426 (86.9%)\n"
     ]
    }
   ],
   "source": [
    "def print_ratio(n, d):\n",
    "    print('{}/{} ({:.1f}%)'.format(n, d, 100 * n/d))\n",
    "\n",
    "def accuracy(x, y, beta):\n",
    "    y_hat = np.round(sigma(x.T @ beta))\n",
    "    guesses = np.round(y_hat)\n",
    "    correct = y == guesses\n",
    "    print_ratio(sum(correct), len(guesses))\n",
    "\n",
    "accuracy(x_train, y_train, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/143 (60.8%)\n"
     ]
    }
   ],
   "source": [
    "x_test = features(test)\n",
    "y_test = test['malignant'].values\n",
    "print_ratio(sum(1-y_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/143 (90.9%)\n"
     ]
    }
   ],
   "source": [
    "accuracy(x_test, y_test, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 406/426 (95.3%)\n",
      "Test: 137/143 (95.8%)\n"
     ]
    }
   ],
   "source": [
    "def all_features(t):\n",
    "    return t.drop('malignant', axis=1).values.T\n",
    "\n",
    "def evaluate(beta, features):\n",
    "    print('Train:', end=' ')\n",
    "    accuracy(features(train), y_train, beta)\n",
    "    print('Test:', end=' ')\n",
    "    accuracy(features(test), y_test, beta)\n",
    "    \n",
    "beta = logistic_regression(all_features(train), y_train)\n",
    "evaluate(beta, all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Logistic Regression\n",
    "\n",
    "As with linear regression, one common way of reducing the variance of the parameter estimator is to add a regularization term to the empirical risk objective. E.g.,\n",
    "\n",
    "\\begin{align*}\n",
    "R(\\beta, x, y, \\lambda) &= - \\frac{1}{n}\\sum_{i=1}^n \\left[ y_i x_i^T\\beta + \\log \\sigma(-x_i^T\\beta) \\right] + \\frac{1}{2} C \\sum_{j=1}^J \\beta_j^2 \\\\[10pt]\n",
    "\\nabla_{\\beta} R(\\beta, x, y, \\lambda) &=  - \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\sigma(x_i^T\\beta)\\right) x_i + C \\beta \\\\[10pt]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 0.0009765625\n",
      "sum(beta**2) =  181.05862166469308\n",
      "Train: 406/426 (95.3%)\n",
      "Test: 137/143 (95.8%)\n",
      "\n",
      "c = 0.00390625\n",
      "sum(beta**2) =  153.17656348299207\n",
      "Train: 406/426 (95.3%)\n",
      "Test: 137/143 (95.8%)\n",
      "\n",
      "c = 0.015625\n",
      "sum(beta**2) =  84.76727676186307\n",
      "Train: 401/426 (94.1%)\n",
      "Test: 139/143 (97.2%)\n",
      "\n",
      "c = 0.0625\n",
      "sum(beta**2) =  16.86808437919907\n",
      "Train: 396/426 (93.0%)\n",
      "Test: 137/143 (95.8%)\n",
      "\n",
      "c = 0.25\n",
      "sum(beta**2) =  1.159763781457247\n",
      "Train: 392/426 (92.0%)\n",
      "Test: 135/143 (94.4%)\n",
      "\n",
      "c = 1.0\n",
      "sum(beta**2) =  0.21983095016705734\n",
      "Train: 390/426 (91.5%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 4.0\n",
      "sum(beta**2) =  0.08526863018405177\n",
      "Train: 390/426 (91.5%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 16.0\n",
      "sum(beta**2) =  0.06240644243334323\n",
      "Train: 390/426 (91.5%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 64.0\n",
      "sum(beta**2) =  0.043885815938907356\n",
      "Train: 388/426 (91.1%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n",
      "c = 256.0\n",
      "sum(beta**2) =  0.016109390288629684\n",
      "Train: 387/426 (90.8%)\n",
      "Test: 136/143 (95.1%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def regularized_logistic_regression(x, y, c):\n",
    "    \"\"\"Train a logistic regression classifier using gradient descent.\"\"\"\n",
    "\n",
    "    def l2_regularized_gradient(beta, x, y):\n",
    "        return risk_gradient(beta, x, y) + c * beta\n",
    "\n",
    "    beta0 = np.zeros(x.shape[0])\n",
    "    beta = gradient_descent(x, y, beta0, l2_regularized_gradient)\n",
    "    return beta    \n",
    "\n",
    "def search_for_c(features):\n",
    "    for c in 2.0 ** np.arange(-10, 10, 2):\n",
    "        print(\"c =\", c)\n",
    "        beta = regularized_logistic_regression(features(train), y_train, c)\n",
    "        print(\"sum(beta**2) = \", sum(beta**2))\n",
    "        evaluate(beta, features)\n",
    "        print()\n",
    "        \n",
    "search_for_c(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 0.0009765625\n",
      "sum(beta**2) =  35.8486017048037\n",
      "Train: 423/426 (99.3%)\n",
      "Test: 138/143 (96.5%)\n",
      "\n",
      "c = 0.00390625\n",
      "sum(beta**2) =  31.318273815172542\n",
      "Train: 423/426 (99.3%)\n",
      "Test: 138/143 (96.5%)\n",
      "\n",
      "c = 0.015625\n",
      "sum(beta**2) =  20.747956964144077\n",
      "Train: 423/426 (99.3%)\n",
      "Test: 139/143 (97.2%)\n",
      "\n",
      "c = 0.0625\n",
      "sum(beta**2) =  9.935156093274612\n",
      "Train: 421/426 (98.8%)\n",
      "Test: 141/143 (98.6%)\n",
      "\n",
      "c = 0.25\n",
      "sum(beta**2) =  4.279626098449614\n",
      "Train: 419/426 (98.4%)\n",
      "Test: 141/143 (98.6%)\n",
      "\n",
      "c = 1.0\n",
      "sum(beta**2) =  1.717220435886093\n",
      "Train: 414/426 (97.2%)\n",
      "Test: 140/143 (97.9%)\n",
      "\n",
      "c = 4.0\n",
      "sum(beta**2) =  0.6094833404635616\n",
      "Train: 412/426 (96.7%)\n",
      "Test: 139/143 (97.2%)\n",
      "\n",
      "c = 16.0\n",
      "sum(beta**2) =  0.17553643009069883\n",
      "Train: 405/426 (95.1%)\n",
      "Test: 138/143 (96.5%)\n",
      "\n",
      "c = 64.0\n",
      "sum(beta**2) =  0.03500234439319296\n",
      "Train: 403/426 (94.6%)\n",
      "Test: 136/143 (95.1%)\n",
      "\n",
      "c = 256.0\n",
      "sum(beta**2) =  0.0041317425176505785\n",
      "Train: 399/426 (93.7%)\n",
      "Test: 134/143 (93.7%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "def inputs(t):\n",
    "    return t.drop('malignant', axis=1).values\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(inputs(train))\n",
    "\n",
    "def scaled_features(t):\n",
    "    return scaler.transform(inputs(t)).T\n",
    "\n",
    "search_for_c(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/143 (96.5%)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=4, solver='lbfgs')\n",
    "model.fit(scaled_features(train).T, y_train)\n",
    "y_hat = model.predict(scaled_features(test).T)\n",
    "print_ratio(sum(y_hat == y_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "\\begin{align*}\n",
    "P(Y=y|X) &= \\frac{\\exp(X^T\\beta_{y})}{\\sum_{z=0}^d \\exp(X^T\\beta_z)} \\\\[10pt]\n",
    "L(\\beta_0,\\dots,\\beta_d, x_i, y_i) &= - \\log \\frac{\\exp(x_i^T\\beta_{y_i})}{\\sum_{z=0}^d \\exp(x_i^T\\beta_z)} \\\\[10pt]\n",
    "\\frac{\\partial}{\\partial \\beta_w} L(\\beta_0,\\dots,\\beta_d, x_i, y_i) &= -\\left(1[w=y_i] - \\frac{\\exp(x_i^T\\beta_w)}{\\sum_{z=0}^d \\exp(x_i^T\\beta_z)}\\right) x_i  \\\\[10pt]\n",
    "1[w=y_i] &= \\begin{cases}\n",
    "1 & \\text{if}\\ w=y_i \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.199333  \n",
       "std            0.762238  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = sklearn.datasets.load_iris()\n",
    "x = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "y = data_dict['target']\n",
    "x.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), array([50, 50, 50]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
